{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - NLP Sentiment Text Analysis v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkim319/Twitter_Sentiment_Analysis/blob/master/1_NLP_Sentiment_Text_Analysis_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H2wGyebN29n",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis - \n",
        "\n",
        "## Introduction\n",
        "\n",
        "This is a Twitter sentiment analysis project to apply the techniques learning in the Deeplearning.ai Tensorflow Developer specialization.  The secondary purpose is to create a template to be used for future NLP classification projects.\n",
        "\n",
        "This project uses NLTK Twitter samples as a dataset.  There were be three separate notebooks and three separate deep learning models to the same problem.\n",
        "\n",
        "1.  Simple DNN model\n",
        "2.  LTSM\n",
        "3.  Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_qjTnmm7xnm",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "All required libraries are loaded and the Tensorflow version is checked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P-AhVYeBWgQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d27ea90-54c9-4435-fb94-2c0da271ad1a"
      },
      "source": [
        "import tensorflow as tf\n",
        "# !pip install -q tensorflow-datasets\n",
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import os\n",
        "from os import getcwd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BThVHlcOV2_w",
        "colab_type": "text"
      },
      "source": [
        "The NLTK Twitter dataset is loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGSoFDDhxBG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "655fb40c-2c3f-4b78-ce54-c73bec086daa"
      },
      "source": [
        "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpnHxqNev1pt",
        "colab_type": "text"
      },
      "source": [
        "This function is created to process the text clean up of tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrXVDaj_x-i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub('@[^\\s]+','',tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CykkpnrOwiMw",
        "colab_type": "text"
      },
      "source": [
        "## 2. Prep Data for Deep Learning\n",
        "\n",
        "The data is split into the training and test datasets (features and labels) and tweets are cleaned up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjOM_v44z44N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_x_cleaned = []\n",
        "test_x_cleaned = []\n",
        "\n",
        "for i, val in enumerate(train_x): \n",
        "   train_x_cleaned.append(process_tweet(train_x[i]))\n",
        "  \n",
        "for i, val in enumerate(test_x): \n",
        "   test_x_cleaned.append(process_tweet(test_x[i]))  \n",
        "\n",
        "# avoid assumptions about the length of all_positive_tweets\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsp2BwzSw_0D",
        "colab_type": "text"
      },
      "source": [
        "Next, the word embedding is created based on the training features and uses post padding and truncation.  Out of vocab token is used to handled words that do not exist in the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7n15yyMdmoH1",
        "colab": {}
      },
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 50\n",
        "padding_type = 'post'\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_x_cleaned)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_x_cleaned)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type,truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxKhr3C1_MBd",
        "colab_type": "text"
      },
      "source": [
        "The tokeinzer is tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YRxoxc2apscY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a5452c7e-ba91-468e-eb3c-10d438bf22d7"
      },
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2], [], [428], [726], [6], [1], [1], [652], [], [421], [859], [2], [371], [765], [], [421], [859], [2], [536], [], [2], [536], [], [6], [665], [6], [1], [2], [371], [632], [], [], [859], [413], [371], [726], [536], [421], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQsmBALsyEjP",
        "colab_type": "text"
      },
      "source": [
        "The embedding vocabulatory is reviewed to see which words are included/excluded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9axf0uIXVMhO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ac87c36-4217-4596-db8e-49f7c29a4e9f"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(padded[1]))\n",
        "print(train_x_cleaned[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hey james how <OOV> please call our <OOV> <OOV> on <OOV> and we will be able to <OOV> you many thanks ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            " Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnqDQazpyQgp",
        "colab_type": "text"
      },
      "source": [
        "## 3. Build the Model\n",
        "\n",
        "\n",
        "The define_model function is created to be able to initialize a word embedding and a model.  It allows for the configuration of the embedding size, model architecture, epochs, and learning rate.  The function also allows for the ability to include a learning rate scheduler and/or a model checkpoint as callbacks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbnd8Nwkz3j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, embedding_dim, max_length, model_number, epoch, learning_rate, lr_cb, cp_cb):\n",
        "\n",
        "  # set the seed and clear session to ensure consistent results and avoid past models impacting the current model\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(319)\n",
        "  np.random.seed(319)\n",
        "\n",
        "  padding_type = 'post'\n",
        "  trunc_type='post'\n",
        "  oov_tok = \"<OOV>\"\n",
        "  \n",
        "  tokenizer = Tokenizer(num_words = vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, oov_token=oov_tok)\n",
        "  tokenizer.fit_on_texts(train_x_cleaned)\n",
        "  word_index = tokenizer.word_index\n",
        "  sequences = tokenizer.texts_to_sequences(train_x_cleaned)\n",
        "  padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type,truncating=trunc_type)\n",
        "\n",
        "  testing_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "  testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
        "\n",
        "  # initialize the callback list\n",
        "  callback_list = []\n",
        "\n",
        "  if model_number==1:\n",
        "    # Define the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "  \n",
        "  if model_number==2:\n",
        "    # Define the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  if model_number==3:\n",
        "    # Define the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(4, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "  if lr_cb==True:\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "        lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "    callback_list.append(lr_schedule)\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-8)\n",
        "\n",
        "  if cp_cb==True:\n",
        "    filepath = str(model_number) + \"_weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "    filedir = os.path.dirname(filepath)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    callback_list.append(checkpoint)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  return tokenizer, model, callback_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6-eTI63z94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chart_acc_loss(model_history):\n",
        "\n",
        "  history = model_history\n",
        "\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "  plt.title('Training and validation accuracy')\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2N6vZVaz_pX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3b1870d-220c-4495-e358-830a653dbbf5"
      },
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 50\n",
        "model_number = 3\n",
        "epoch = 200\n",
        "learning_rate = 0.001\n",
        "lr_cb = True\n",
        "cp_cb = False\n",
        "\n",
        "tokenizer, model_lr_sched, callback_list = define_model(vocab_size, embedding_dim, max_length, model_number, epoch, learning_rate , lr_cb, cp_cb)\n",
        "\n",
        "history = model_lr_sched.fit(padded, train_y, \n",
        "                  epochs=epoch, \n",
        "                  validation_data = (testing_padded, test_y),\n",
        "                  callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 16)            16000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 3204      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 19,209\n",
            "Trainable params: 19,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4780\n",
            "Epoch 2/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4780\n",
            "Epoch 3/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4785\n",
            "Epoch 4/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4785\n",
            "Epoch 5/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4785\n",
            "Epoch 6/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4780\n",
            "Epoch 7/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 8/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 9/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 10/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 11/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 12/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 13/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4939 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 14/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 15/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4780\n",
            "Epoch 16/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4780\n",
            "Epoch 17/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4940 - val_loss: 0.6940 - val_accuracy: 0.4780\n",
            "Epoch 18/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.4780\n",
            "Epoch 19/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4936 - val_loss: 0.6940 - val_accuracy: 0.4775\n",
            "Epoch 20/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4935 - val_loss: 0.6940 - val_accuracy: 0.4770\n",
            "Epoch 21/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4938 - val_loss: 0.6939 - val_accuracy: 0.4765\n",
            "Epoch 22/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6939 - val_accuracy: 0.4770\n",
            "Epoch 23/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4942 - val_loss: 0.6939 - val_accuracy: 0.4770\n",
            "Epoch 24/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6939 - val_accuracy: 0.4770\n",
            "Epoch 25/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4942 - val_loss: 0.6939 - val_accuracy: 0.4770\n",
            "Epoch 26/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4938 - val_loss: 0.6939 - val_accuracy: 0.4765\n",
            "Epoch 27/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4938 - val_loss: 0.6939 - val_accuracy: 0.4760\n",
            "Epoch 28/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4935 - val_loss: 0.6939 - val_accuracy: 0.4760\n",
            "Epoch 29/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4936 - val_loss: 0.6939 - val_accuracy: 0.4755\n",
            "Epoch 30/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4939 - val_loss: 0.6939 - val_accuracy: 0.4750\n",
            "Epoch 31/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4941 - val_loss: 0.6939 - val_accuracy: 0.4740\n",
            "Epoch 32/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4945 - val_loss: 0.6939 - val_accuracy: 0.4740\n",
            "Epoch 33/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4945 - val_loss: 0.6939 - val_accuracy: 0.4740\n",
            "Epoch 34/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4947 - val_loss: 0.6939 - val_accuracy: 0.4735\n",
            "Epoch 35/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4949 - val_loss: 0.6939 - val_accuracy: 0.4730\n",
            "Epoch 36/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4949 - val_loss: 0.6939 - val_accuracy: 0.4730\n",
            "Epoch 37/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4955 - val_loss: 0.6939 - val_accuracy: 0.4735\n",
            "Epoch 38/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4958 - val_loss: 0.6939 - val_accuracy: 0.4730\n",
            "Epoch 39/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4964 - val_loss: 0.6938 - val_accuracy: 0.4740\n",
            "Epoch 40/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4964 - val_loss: 0.6938 - val_accuracy: 0.4725\n",
            "Epoch 41/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4966 - val_loss: 0.6938 - val_accuracy: 0.4740\n",
            "Epoch 42/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6938 - val_accuracy: 0.4740\n",
            "Epoch 43/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6938 - val_accuracy: 0.4745\n",
            "Epoch 44/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4971 - val_loss: 0.6938 - val_accuracy: 0.4750\n",
            "Epoch 45/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4972 - val_loss: 0.6938 - val_accuracy: 0.4730\n",
            "Epoch 46/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6938 - val_accuracy: 0.4700\n",
            "Epoch 47/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4978 - val_loss: 0.6937 - val_accuracy: 0.4670\n",
            "Epoch 48/200\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6937 - val_accuracy: 0.4620\n",
            "Epoch 49/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.4979 - val_loss: 0.6937 - val_accuracy: 0.4615\n",
            "Epoch 50/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.4989 - val_loss: 0.6937 - val_accuracy: 0.4625\n",
            "Epoch 51/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.4991 - val_loss: 0.6937 - val_accuracy: 0.4605\n",
            "Epoch 52/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5021 - val_loss: 0.6937 - val_accuracy: 0.4595\n",
            "Epoch 53/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5015 - val_loss: 0.6937 - val_accuracy: 0.4600\n",
            "Epoch 54/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5034 - val_loss: 0.6937 - val_accuracy: 0.4590\n",
            "Epoch 55/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5049 - val_loss: 0.6937 - val_accuracy: 0.4545\n",
            "Epoch 56/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5102 - val_loss: 0.6937 - val_accuracy: 0.4530\n",
            "Epoch 57/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5210 - val_loss: 0.6937 - val_accuracy: 0.4535\n",
            "Epoch 58/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5303 - val_loss: 0.6937 - val_accuracy: 0.4580\n",
            "Epoch 59/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5374 - val_loss: 0.6937 - val_accuracy: 0.4665\n",
            "Epoch 60/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5519 - val_loss: 0.6936 - val_accuracy: 0.4770\n",
            "Epoch 61/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5639 - val_loss: 0.6936 - val_accuracy: 0.4835\n",
            "Epoch 62/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5695 - val_loss: 0.6936 - val_accuracy: 0.4840\n",
            "Epoch 63/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5774 - val_loss: 0.6936 - val_accuracy: 0.4775\n",
            "Epoch 64/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5832 - val_loss: 0.6937 - val_accuracy: 0.4665\n",
            "Epoch 65/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5978 - val_loss: 0.6939 - val_accuracy: 0.4595\n",
            "Epoch 66/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.6069 - val_loss: 0.6940 - val_accuracy: 0.4660\n",
            "Epoch 67/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.6151 - val_loss: 0.6940 - val_accuracy: 0.4605\n",
            "Epoch 68/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.6317 - val_loss: 0.6940 - val_accuracy: 0.4620\n",
            "Epoch 69/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6859 - accuracy: 0.6388 - val_loss: 0.6940 - val_accuracy: 0.4660\n",
            "Epoch 70/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6840 - accuracy: 0.6591 - val_loss: 0.6940 - val_accuracy: 0.4680\n",
            "Epoch 71/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6815 - accuracy: 0.6653 - val_loss: 0.6940 - val_accuracy: 0.4675\n",
            "Epoch 72/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.6839 - val_loss: 0.6940 - val_accuracy: 0.4670\n",
            "Epoch 73/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6740 - accuracy: 0.6920 - val_loss: 0.6940 - val_accuracy: 0.4660\n",
            "Epoch 74/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6682 - accuracy: 0.7099 - val_loss: 0.6939 - val_accuracy: 0.4585\n",
            "Epoch 75/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6603 - accuracy: 0.7284 - val_loss: 0.6941 - val_accuracy: 0.4600\n",
            "Epoch 76/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6498 - accuracy: 0.7369 - val_loss: 0.6947 - val_accuracy: 0.4810\n",
            "Epoch 77/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.7355 - val_loss: 0.6945 - val_accuracy: 0.4740\n",
            "Epoch 78/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6185 - accuracy: 0.7435 - val_loss: 0.6946 - val_accuracy: 0.4630\n",
            "Epoch 79/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.7571 - val_loss: 0.6945 - val_accuracy: 0.4995\n",
            "Epoch 80/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5746 - accuracy: 0.7631 - val_loss: 0.6949 - val_accuracy: 0.4755\n",
            "Epoch 81/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.7697 - val_loss: 0.6966 - val_accuracy: 0.4990\n",
            "Epoch 82/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7770 - val_loss: 0.6962 - val_accuracy: 0.5025\n",
            "Epoch 83/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7810 - val_loss: 0.7001 - val_accuracy: 0.5000\n",
            "Epoch 84/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7868 - val_loss: 0.6969 - val_accuracy: 0.5035\n",
            "Epoch 85/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7961 - val_loss: 0.6983 - val_accuracy: 0.5005\n",
            "Epoch 86/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8035 - val_loss: 0.7079 - val_accuracy: 0.5000\n",
            "Epoch 87/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8123 - val_loss: 0.7085 - val_accuracy: 0.5000\n",
            "Epoch 88/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.8221 - val_loss: 0.7114 - val_accuracy: 0.5000\n",
            "Epoch 89/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8290 - val_loss: 0.7351 - val_accuracy: 0.5000\n",
            "Epoch 90/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3742 - accuracy: 0.8378 - val_loss: 0.7190 - val_accuracy: 0.5000\n",
            "Epoch 91/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8464 - val_loss: 0.7340 - val_accuracy: 0.5000\n",
            "Epoch 92/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8537 - val_loss: 0.7346 - val_accuracy: 0.5000\n",
            "Epoch 93/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8601 - val_loss: 0.7197 - val_accuracy: 0.4960\n",
            "Epoch 94/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8710 - val_loss: 0.7311 - val_accuracy: 0.4965\n",
            "Epoch 95/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2955 - accuracy: 0.8796 - val_loss: 0.7241 - val_accuracy: 0.4765\n",
            "Epoch 96/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2778 - accuracy: 0.8875 - val_loss: 0.7257 - val_accuracy: 0.3650\n",
            "Epoch 97/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.8956 - val_loss: 0.7359 - val_accuracy: 0.3650\n",
            "Epoch 98/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.9054 - val_loss: 0.7625 - val_accuracy: 0.3285\n",
            "Epoch 99/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9158 - val_loss: 0.7736 - val_accuracy: 0.2975\n",
            "Epoch 100/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2070 - accuracy: 0.9226 - val_loss: 0.8024 - val_accuracy: 0.3055\n",
            "Epoch 101/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1888 - accuracy: 0.9268 - val_loss: 0.8675 - val_accuracy: 0.4105\n",
            "Epoch 102/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9331 - val_loss: 0.8957 - val_accuracy: 0.2710\n",
            "Epoch 103/200\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9395 - val_loss: 1.0062 - val_accuracy: 0.3580\n",
            "Epoch 104/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1454 - accuracy: 0.9417 - val_loss: 1.0453 - val_accuracy: 0.2650\n",
            "Epoch 105/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1304 - accuracy: 0.9480 - val_loss: 1.1998 - val_accuracy: 0.3315\n",
            "Epoch 106/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1207 - accuracy: 0.9510 - val_loss: 1.3431 - val_accuracy: 0.3065\n",
            "Epoch 107/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9539 - val_loss: 1.4702 - val_accuracy: 0.2935\n",
            "Epoch 108/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.9556 - val_loss: 1.6825 - val_accuracy: 0.2625\n",
            "Epoch 109/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1014 - accuracy: 0.9596 - val_loss: 1.9337 - val_accuracy: 0.2620\n",
            "Epoch 110/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1016 - accuracy: 0.9563 - val_loss: 2.0663 - val_accuracy: 0.2760\n",
            "Epoch 111/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1009 - accuracy: 0.9575 - val_loss: 2.2864 - val_accuracy: 0.2600\n",
            "Epoch 112/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0993 - accuracy: 0.9574 - val_loss: 2.6234 - val_accuracy: 0.3095\n",
            "Epoch 113/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9541 - val_loss: 2.6760 - val_accuracy: 0.2815\n",
            "Epoch 114/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9491 - val_loss: 2.1507 - val_accuracy: 0.2670\n",
            "Epoch 115/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1260 - accuracy: 0.9442 - val_loss: 2.4134 - val_accuracy: 0.2620\n",
            "Epoch 116/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1189 - accuracy: 0.9469 - val_loss: 2.7668 - val_accuracy: 0.2550\n",
            "Epoch 117/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.9536 - val_loss: 2.9155 - val_accuracy: 0.2510\n",
            "Epoch 118/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9491 - val_loss: 2.8630 - val_accuracy: 0.2940\n",
            "Epoch 119/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9510 - val_loss: 2.4742 - val_accuracy: 0.2825\n",
            "Epoch 120/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1159 - accuracy: 0.9525 - val_loss: 2.5418 - val_accuracy: 0.2630\n",
            "Epoch 121/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9492 - val_loss: 2.9048 - val_accuracy: 0.2760\n",
            "Epoch 122/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9473 - val_loss: 2.9581 - val_accuracy: 0.2535\n",
            "Epoch 123/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.9519 - val_loss: 3.2204 - val_accuracy: 0.2520\n",
            "Epoch 124/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9498 - val_loss: 2.8894 - val_accuracy: 0.2710\n",
            "Epoch 125/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9405 - val_loss: 3.2601 - val_accuracy: 0.2550\n",
            "Epoch 126/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9429 - val_loss: 2.9871 - val_accuracy: 0.2585\n",
            "Epoch 127/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9476 - val_loss: 3.1514 - val_accuracy: 0.2680\n",
            "Epoch 128/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9465 - val_loss: 2.7643 - val_accuracy: 0.2715\n",
            "Epoch 129/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9473 - val_loss: 3.5828 - val_accuracy: 0.2695\n",
            "Epoch 130/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9426 - val_loss: 2.5257 - val_accuracy: 0.2610\n",
            "Epoch 131/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1579 - accuracy: 0.9320 - val_loss: 2.1485 - val_accuracy: 0.2570\n",
            "Epoch 132/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9326 - val_loss: 2.9305 - val_accuracy: 0.2570\n",
            "Epoch 133/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1630 - accuracy: 0.9316 - val_loss: 3.7257 - val_accuracy: 0.2465\n",
            "Epoch 134/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.9329 - val_loss: 3.4898 - val_accuracy: 0.3010\n",
            "Epoch 135/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.9296 - val_loss: 3.1405 - val_accuracy: 0.4225\n",
            "Epoch 136/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.8882 - val_loss: 1.6664 - val_accuracy: 0.4440\n",
            "Epoch 137/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.8054 - val_loss: 7.5866 - val_accuracy: 0.2710\n",
            "Epoch 138/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.6977 - val_loss: 1.3554 - val_accuracy: 0.4970\n",
            "Epoch 139/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.6524 - val_loss: 6.2950 - val_accuracy: 0.4405\n",
            "Epoch 140/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.5626 - val_loss: 25.1041 - val_accuracy: 0.4995\n",
            "Epoch 141/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.4908 - val_loss: 25.1031 - val_accuracy: 0.4995\n",
            "Epoch 142/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.4956 - val_loss: 25.0917 - val_accuracy: 0.4970\n",
            "Epoch 143/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6956 - accuracy: 0.5052 - val_loss: 25.1114 - val_accuracy: 0.4995\n",
            "Epoch 144/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6963 - accuracy: 0.4979 - val_loss: 25.0972 - val_accuracy: 0.4970\n",
            "Epoch 145/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6946 - accuracy: 0.5001 - val_loss: 25.1173 - val_accuracy: 0.4995\n",
            "Epoch 146/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6964 - accuracy: 0.4952 - val_loss: 25.1062 - val_accuracy: 0.4995\n",
            "Epoch 147/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.4979 - val_loss: 25.0896 - val_accuracy: 0.4970\n",
            "Epoch 148/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6968 - accuracy: 0.4996 - val_loss: 25.1279 - val_accuracy: 0.4995\n",
            "Epoch 149/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6996 - accuracy: 0.4967 - val_loss: 25.0981 - val_accuracy: 0.4970\n",
            "Epoch 150/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6993 - accuracy: 0.4959 - val_loss: 25.0962 - val_accuracy: 0.4970\n",
            "Epoch 151/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.4981 - val_loss: 25.1303 - val_accuracy: 0.4995\n",
            "Epoch 152/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7001 - accuracy: 0.4933 - val_loss: 25.0915 - val_accuracy: 0.4970\n",
            "Epoch 153/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7030 - accuracy: 0.5029 - val_loss: 25.0978 - val_accuracy: 0.4970\n",
            "Epoch 154/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6999 - accuracy: 0.4972 - val_loss: 25.0889 - val_accuracy: 0.4970\n",
            "Epoch 155/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.6998 - accuracy: 0.4995 - val_loss: 25.0956 - val_accuracy: 0.4970\n",
            "Epoch 156/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7037 - accuracy: 0.4983 - val_loss: 25.1150 - val_accuracy: 0.4995\n",
            "Epoch 157/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7143 - accuracy: 0.5027 - val_loss: 25.0935 - val_accuracy: 0.4970\n",
            "Epoch 158/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.5071 - val_loss: 25.1043 - val_accuracy: 0.4995\n",
            "Epoch 159/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7156 - accuracy: 0.5005 - val_loss: 25.1082 - val_accuracy: 0.4995\n",
            "Epoch 160/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7087 - accuracy: 0.4854 - val_loss: 25.1058 - val_accuracy: 0.4995\n",
            "Epoch 161/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7104 - accuracy: 0.4888 - val_loss: 25.0915 - val_accuracy: 0.4970\n",
            "Epoch 162/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7111 - accuracy: 0.4952 - val_loss: 25.0889 - val_accuracy: 0.4970\n",
            "Epoch 163/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7165 - accuracy: 0.5009 - val_loss: 25.1882 - val_accuracy: 0.4995\n",
            "Epoch 164/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7184 - accuracy: 0.5130 - val_loss: 25.0891 - val_accuracy: 0.4970\n",
            "Epoch 165/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7115 - accuracy: 0.5110 - val_loss: 25.3170 - val_accuracy: 0.4995\n",
            "Epoch 166/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7195 - accuracy: 0.5049 - val_loss: 25.1314 - val_accuracy: 0.4995\n",
            "Epoch 167/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7249 - accuracy: 0.5026 - val_loss: 25.0953 - val_accuracy: 0.4970\n",
            "Epoch 168/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7233 - accuracy: 0.4979 - val_loss: 25.2223 - val_accuracy: 0.4995\n",
            "Epoch 169/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7435 - accuracy: 0.4974 - val_loss: 25.0920 - val_accuracy: 0.4970\n",
            "Epoch 170/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7510 - accuracy: 0.5077 - val_loss: 25.1055 - val_accuracy: 0.4995\n",
            "Epoch 171/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7368 - accuracy: 0.5102 - val_loss: 25.1019 - val_accuracy: 0.4970\n",
            "Epoch 172/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7461 - accuracy: 0.4978 - val_loss: 25.1013 - val_accuracy: 0.4970\n",
            "Epoch 173/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7692 - accuracy: 0.4981 - val_loss: 25.0899 - val_accuracy: 0.4970\n",
            "Epoch 174/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7616 - accuracy: 0.5023 - val_loss: 25.1323 - val_accuracy: 0.4970\n",
            "Epoch 175/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7360 - accuracy: 0.5020 - val_loss: 25.1097 - val_accuracy: 0.4995\n",
            "Epoch 176/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7780 - accuracy: 0.4947 - val_loss: 25.1163 - val_accuracy: 0.4995\n",
            "Epoch 177/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7964 - accuracy: 0.5042 - val_loss: 25.2345 - val_accuracy: 0.4975\n",
            "Epoch 178/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7789 - accuracy: 0.5071 - val_loss: 25.1387 - val_accuracy: 0.4970\n",
            "Epoch 179/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.8086 - accuracy: 0.4924 - val_loss: 25.1163 - val_accuracy: 0.4970\n",
            "Epoch 180/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.9185 - accuracy: 0.4952 - val_loss: 25.1292 - val_accuracy: 0.4995\n",
            "Epoch 181/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.8253 - accuracy: 0.5010 - val_loss: 25.0993 - val_accuracy: 0.4970\n",
            "Epoch 182/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.7821 - accuracy: 0.4997 - val_loss: 25.1027 - val_accuracy: 0.4995\n",
            "Epoch 183/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.8005 - accuracy: 0.4960 - val_loss: 25.7499 - val_accuracy: 0.4995\n",
            "Epoch 184/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.8562 - accuracy: 0.5034 - val_loss: 25.1354 - val_accuracy: 0.4995\n",
            "Epoch 185/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.9170 - accuracy: 0.4942 - val_loss: 25.0910 - val_accuracy: 0.4970\n",
            "Epoch 186/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 1.2099 - accuracy: 0.5027 - val_loss: 25.1143 - val_accuracy: 0.4970\n",
            "Epoch 187/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 1.0656 - accuracy: 0.4955 - val_loss: 25.0927 - val_accuracy: 0.4970\n",
            "Epoch 188/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.9555 - accuracy: 0.5079 - val_loss: 25.0997 - val_accuracy: 0.4970\n",
            "Epoch 189/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.9581 - accuracy: 0.4884 - val_loss: 25.8637 - val_accuracy: 0.4975\n",
            "Epoch 190/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.9931 - accuracy: 0.5079 - val_loss: 25.0944 - val_accuracy: 0.4970\n",
            "Epoch 191/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 1.6815 - accuracy: 0.5004 - val_loss: 25.8850 - val_accuracy: 0.4975\n",
            "Epoch 192/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 1.8459 - accuracy: 0.5020 - val_loss: 25.4719 - val_accuracy: 0.4975\n",
            "Epoch 193/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 1.7105 - accuracy: 0.4989 - val_loss: 25.2853 - val_accuracy: 0.4975\n",
            "Epoch 194/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 2.1400 - accuracy: 0.5065 - val_loss: 25.1168 - val_accuracy: 0.4970\n",
            "Epoch 195/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 2.2444 - accuracy: 0.4981 - val_loss: 25.5152 - val_accuracy: 0.4975\n",
            "Epoch 196/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 2.7640 - accuracy: 0.5070 - val_loss: 25.0990 - val_accuracy: 0.4970\n",
            "Epoch 197/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 3.3574 - accuracy: 0.4997 - val_loss: 25.6482 - val_accuracy: 0.4995\n",
            "Epoch 198/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 2.8464 - accuracy: 0.5017 - val_loss: 26.0124 - val_accuracy: 0.4995\n",
            "Epoch 199/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 5.6843 - accuracy: 0.5066 - val_loss: 29.4453 - val_accuracy: 0.4970\n",
            "Epoch 200/200\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 3.9868 - accuracy: 0.5023 - val_loss: 25.9861 - val_accuracy: 0.4995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVu8XM9D1ddJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "caad3ef5-2bbc-4cf0-b338-5220da7f3f18"
      },
      "source": [
        "# review the learning rate performance\n",
        "\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZdD2jo13DT_",
        "colab_type": "text"
      },
      "source": [
        "Based on the chart, the learning rate of 0.01(10e-3) appears to be the best option, but when a model is trained using this learning rate.  The accuracy and loss charts show instability, which means that the learning rate is probably too high."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ks0xH8S3BN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7739ec07-bc51-4823-c838-8691fc0ba4a8"
      },
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 50\n",
        "model_number = 3\n",
        "epoch = 20\n",
        "learning_rate = 0.001\n",
        "lr_cb = False\n",
        "cp_cb = True\n",
        "\n",
        "tokenizer, model_lr_sched, callback_list = define_model(vocab_size, embedding_dim, max_length,model_number, epoch, learning_rate , lr_cb, cp_cb)\n",
        "\n",
        "history = model_lr_sched.fit(padded, train_y, \n",
        "                  epochs=epoch, \n",
        "                  validation_data = (testing_padded, test_y),\n",
        "                  callbacks=[callback_list])\n",
        "\n",
        "# review the learning rate performance\n",
        "chart_acc_loss(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 16)            16000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 3204      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 19,209\n",
            "Trainable params: 19,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "237/250 [===========================>..] - ETA: 0s - loss: 0.6568 - accuracy: 0.6262\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.49050, saving model to 3_weights-improvement-01-0.49.hdf5\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 0.6512 - accuracy: 0.6326 - val_loss: 0.7151 - val_accuracy: 0.4905\n",
            "Epoch 2/20\n",
            "242/250 [============================>.] - ETA: 0s - loss: 0.4799 - accuracy: 0.7761\n",
            "Epoch 00002: val_accuracy improved from 0.49050 to 0.50000, saving model to 3_weights-improvement-02-0.50.hdf5\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7759 - val_loss: 0.7823 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "225/250 [==========================>...] - ETA: 0s - loss: 0.4039 - accuracy: 0.8153\n",
            "Epoch 00003: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8180 - val_loss: 0.8771 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "224/250 [=========================>....] - ETA: 0s - loss: 0.3539 - accuracy: 0.8443\n",
            "Epoch 00004: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8425 - val_loss: 0.7784 - val_accuracy: 0.2995\n",
            "Epoch 5/20\n",
            "241/250 [===========================>..] - ETA: 0s - loss: 0.3183 - accuracy: 0.8641\n",
            "Epoch 00005: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8633 - val_loss: 0.8604 - val_accuracy: 0.4840\n",
            "Epoch 6/20\n",
            "230/250 [==========================>...] - ETA: 0s - loss: 0.2864 - accuracy: 0.8788\n",
            "Epoch 00006: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8789 - val_loss: 0.8926 - val_accuracy: 0.4655\n",
            "Epoch 7/20\n",
            "233/250 [==========================>...] - ETA: 0s - loss: 0.2528 - accuracy: 0.9006\n",
            "Epoch 00007: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.8997 - val_loss: 0.8850 - val_accuracy: 0.3825\n",
            "Epoch 8/20\n",
            "244/250 [============================>.] - ETA: 0s - loss: 0.2255 - accuracy: 0.9139\n",
            "Epoch 00008: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9139 - val_loss: 0.9146 - val_accuracy: 0.2995\n",
            "Epoch 9/20\n",
            "230/250 [==========================>...] - ETA: 0s - loss: 0.2013 - accuracy: 0.9273\n",
            "Epoch 00009: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9254 - val_loss: 0.9810 - val_accuracy: 0.2805\n",
            "Epoch 10/20\n",
            "237/250 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9338\n",
            "Epoch 00010: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1807 - accuracy: 0.9329 - val_loss: 1.0931 - val_accuracy: 0.3790\n",
            "Epoch 11/20\n",
            "236/250 [===========================>..] - ETA: 0s - loss: 0.1626 - accuracy: 0.9396\n",
            "Epoch 00011: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1630 - accuracy: 0.9399 - val_loss: 1.1316 - val_accuracy: 0.3275\n",
            "Epoch 12/20\n",
            "234/250 [===========================>..] - ETA: 0s - loss: 0.1472 - accuracy: 0.9448\n",
            "Epoch 00012: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9442 - val_loss: 1.2034 - val_accuracy: 0.3310\n",
            "Epoch 13/20\n",
            "232/250 [==========================>...] - ETA: 0s - loss: 0.1331 - accuracy: 0.9502\n",
            "Epoch 00013: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1331 - accuracy: 0.9494 - val_loss: 1.2748 - val_accuracy: 0.2725\n",
            "Epoch 14/20\n",
            "227/250 [==========================>...] - ETA: 0s - loss: 0.1208 - accuracy: 0.9543\n",
            "Epoch 00014: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9525 - val_loss: 1.3833 - val_accuracy: 0.2745\n",
            "Epoch 15/20\n",
            "238/250 [===========================>..] - ETA: 0s - loss: 0.1128 - accuracy: 0.9550\n",
            "Epoch 00015: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.9553 - val_loss: 1.4424 - val_accuracy: 0.2870\n",
            "Epoch 16/20\n",
            "237/250 [===========================>..] - ETA: 0s - loss: 0.1040 - accuracy: 0.9583\n",
            "Epoch 00016: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9576 - val_loss: 1.6222 - val_accuracy: 0.2780\n",
            "Epoch 17/20\n",
            "235/250 [===========================>..] - ETA: 0s - loss: 0.0961 - accuracy: 0.9624\n",
            "Epoch 00017: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.9614 - val_loss: 1.6775 - val_accuracy: 0.2785\n",
            "Epoch 18/20\n",
            "230/250 [==========================>...] - ETA: 0s - loss: 0.0916 - accuracy: 0.9647\n",
            "Epoch 00018: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0928 - accuracy: 0.9641 - val_loss: 1.8240 - val_accuracy: 0.2640\n",
            "Epoch 19/20\n",
            "228/250 [==========================>...] - ETA: 0s - loss: 0.0854 - accuracy: 0.9661\n",
            "Epoch 00019: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0866 - accuracy: 0.9664 - val_loss: 1.9184 - val_accuracy: 0.2615\n",
            "Epoch 20/20\n",
            "235/250 [===========================>..] - ETA: 0s - loss: 0.0811 - accuracy: 0.9674\n",
            "Epoch 00020: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9653 - val_loss: 1.9911 - val_accuracy: 0.2590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL_A3UEF_X46",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The simple DNN model performed poorly against the validation data.  As the model started overfitting, the validation accuracy went down.  This is indicative that the training dataset is not large or sufficient enough to produce a generalized model.  The second notebook will try a different DNN architecture: LTSM."
      ]
    }
  ]
}