{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - NLP Sentiment Text Analysis - LTSM v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkim319/Twitter_Sentiment_Analysis/blob/master/2_NLP_Sentiment_Text_Analysis_LTSM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_qjTnmm7xnm",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis - LTSM\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook covers the tuning and building of a LTSM model for Twitter sentiment analysis of the NLTK Twitter samples as a dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqvivVrfEkMp",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "All required libraries are loaded and the Tensorflow version is checked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVf-McFKwWF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe4b3b05-8830-45ed-aadc-3c3c214623cb"
      },
      "source": [
        "import tensorflow as tf\n",
        "# !pip install -q tensorflow-datasets\n",
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import os\n",
        "from os import getcwd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmFLN6eJU6NW",
        "colab_type": "text"
      },
      "source": [
        "The NLTK Twitter dataset is loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGSoFDDhxBG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "913e5556-5202-42bf-f7a0-4f4f6a0dae14"
      },
      "source": [
        "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6BkL0JfVA5k",
        "colab_type": "text"
      },
      "source": [
        "## 2. Prep Data for Deep Learning\n",
        "\n",
        "The data is split into the training and test datasets (features and labels) and tweets are cleaned up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffd-kYTDj3wA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub('@[^\\s]+','',tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16CSO1MOExGG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Prep Data for Deep Learning\n",
        "\n",
        "The data is split into the training and test datasets (features and labels) and tweets are cleaned up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KYwHhekhgqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_x_cleaned = []\n",
        "test_x_cleaned = []\n",
        "\n",
        "for i, val in enumerate(train_x): \n",
        "   train_x_cleaned.append(process_tweet(train_x[i]))\n",
        "  \n",
        "for i, val in enumerate(test_x): \n",
        "   test_x_cleaned.append(process_tweet(test_x[i]))  \n",
        "\n",
        "# avoid assumptions about the length of all_positive_tweets\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkbQjbVkiBLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 32\n",
        "max_length = 50\n",
        "padding_type = 'post'\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_x_cleaned)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_x_cleaned)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type,truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhZMCj2uE6qs",
        "colab_type": "text"
      },
      "source": [
        "The tokeinzer is tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOhQ9EA6uf-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ba916489-310e-4964-d35e-5d2a842187ea"
      },
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2], [], [428], [726], [6], [1], [1], [652], [], [421], [859], [2], [371], [765], [], [421], [859], [2], [536], [], [2], [536], [], [6], [665], [6], [1], [2], [371], [632], [], [], [859], [413], [371], [726], [536], [421], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDy0vrLfFAMq",
        "colab_type": "text"
      },
      "source": [
        "The embedding vocabulatory is reviewed to see which words are included/excluded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meWhCq9Jjp2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0cf026d3-f57b-41de-d62d-744bad922027"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(padded[1]))\n",
        "print(train_x_cleaned[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hey james how <OOV> please call our <OOV> <OOV> on <OOV> and we will be able to <OOV> you many thanks ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            " Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8G8XKlbGwuC",
        "colab_type": "text"
      },
      "source": [
        "## 3. Build the Model\n",
        "\n",
        "The define_model function is created to be able to initialize a word embedding and a model.  It allows for the configuration of the embedding size, model architecture, epochs, and learning rate.  The function also allows for the ability to include a learning rate scheduler and/or a model checkpoint as callbacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ0H3YT6T5sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, embedding_dim, max_length, model_number, epoch, learning_rate, lr_cb, cp_cb):\n",
        "\n",
        "  # set the seed and clear session to ensure consistent results and avoid past models impacting the current model\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(319)\n",
        "  np.random.seed(319)\n",
        "\n",
        "  padding_type = 'post'\n",
        "  trunc_type='post'\n",
        "  oov_tok = \"<OOV>\"\n",
        "  \n",
        "  tokenizer = Tokenizer(num_words = vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, oov_token=oov_tok)\n",
        "  tokenizer.fit_on_texts(train_x_cleaned)\n",
        "  word_index = tokenizer.word_index\n",
        "  sequences = tokenizer.texts_to_sequences(train_x_cleaned)\n",
        "  padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type,truncating=trunc_type)\n",
        "\n",
        "  testing_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "  testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
        "  \n",
        "  # initialize the callback list\n",
        "  callback_list = []\n",
        "\n",
        "  if model_number==1:\n",
        "    # Define the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "        #tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "  \n",
        "  if model_number==2:\n",
        "    # Define the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(6, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "  if lr_cb==True:\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "        lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "    callback_list.append(lr_schedule)\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-8)\n",
        "\n",
        "  if cp_cb==True:\n",
        "    filepath = str(model_number) + \"_weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "    filedir = os.path.dirname(filepath)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    callback_list.append(checkpoint)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  return tokenizer, model, callback_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHCaHtXLUYGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chart_acc_loss(model_history):\n",
        "\n",
        "  history = model_history\n",
        "\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "  plt.title('Training and validation accuracy')\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5LLrXC-uNX6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0c4dedb-4a8c-4e3c-f381-c9e871579f5a"
      },
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 32\n",
        "max_length = 50\n",
        "model_number = 1\n",
        "epoch = 100\n",
        "learning_rate = 0.001\n",
        "lr_cb = True\n",
        "cp_cb = False\n",
        "\n",
        "tokenizer, model_lr_sched, callback_list = define_model(vocab_size, embedding_dim, max_length, model_number, epoch, learning_rate , lr_cb, cp_cb)\n",
        "\n",
        "history = model_lr_sched.fit(padded, train_y, \n",
        "                  epochs=epoch, \n",
        "                  validation_data = (testing_padded, test_y),\n",
        "                  callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 32)            32000     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               164864    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 229,889\n",
            "Trainable params: 229,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.6931 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.4959 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.4963 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.4966 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.4972 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.4978 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.4991 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.4991 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5026 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5041 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.5055 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5070 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.5092 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6931 - accuracy: 0.5101 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5123 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5170 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6931 - accuracy: 0.5206 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6930 - accuracy: 0.5226 - val_loss: 0.6932 - val_accuracy: 0.5005\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6930 - accuracy: 0.5244 - val_loss: 0.6931 - val_accuracy: 0.5005\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6930 - accuracy: 0.5289 - val_loss: 0.6931 - val_accuracy: 0.5005\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6930 - accuracy: 0.5346 - val_loss: 0.6931 - val_accuracy: 0.5005\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6930 - accuracy: 0.5387 - val_loss: 0.6931 - val_accuracy: 0.5005\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6930 - accuracy: 0.5400 - val_loss: 0.6931 - val_accuracy: 0.5005\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6929 - accuracy: 0.5435 - val_loss: 0.6931 - val_accuracy: 0.5005\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6929 - accuracy: 0.5461 - val_loss: 0.6931 - val_accuracy: 0.5005\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6929 - accuracy: 0.5493 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6929 - accuracy: 0.5500 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6928 - accuracy: 0.5579 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6928 - accuracy: 0.5612 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6927 - accuracy: 0.5639 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6927 - accuracy: 0.5680 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6926 - accuracy: 0.5724 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6926 - accuracy: 0.5782 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6925 - accuracy: 0.5845 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6924 - accuracy: 0.5860 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6923 - accuracy: 0.5920 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6921 - accuracy: 0.5954 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6920 - accuracy: 0.5986 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6917 - accuracy: 0.6011 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6914 - accuracy: 0.6045 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6911 - accuracy: 0.6129 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6906 - accuracy: 0.6221 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6899 - accuracy: 0.6302 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6889 - accuracy: 0.6363 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6874 - accuracy: 0.6476 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6847 - accuracy: 0.6574 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6803 - accuracy: 0.6747 - val_loss: 0.6916 - val_accuracy: 0.5195\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6721 - accuracy: 0.6873 - val_loss: 0.6794 - val_accuracy: 0.6150\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6583 - accuracy: 0.6758 - val_loss: 0.6372 - val_accuracy: 0.6655\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6388 - accuracy: 0.6911 - val_loss: 0.5729 - val_accuracy: 0.6930\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6189 - accuracy: 0.6869 - val_loss: 0.5595 - val_accuracy: 0.7115\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5987 - accuracy: 0.7104 - val_loss: 0.5636 - val_accuracy: 0.7215\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5790 - accuracy: 0.7314 - val_loss: 0.5854 - val_accuracy: 0.7090\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5633 - accuracy: 0.7356 - val_loss: 0.5668 - val_accuracy: 0.7170\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5454 - accuracy: 0.7415 - val_loss: 0.6018 - val_accuracy: 0.6660\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5276 - accuracy: 0.7570 - val_loss: 0.6390 - val_accuracy: 0.6315\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5074 - accuracy: 0.7649 - val_loss: 0.8404 - val_accuracy: 0.5135\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4815 - accuracy: 0.7778 - val_loss: 0.9832 - val_accuracy: 0.5045\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4572 - accuracy: 0.7868 - val_loss: 0.8969 - val_accuracy: 0.5070\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4395 - accuracy: 0.7991 - val_loss: 0.8805 - val_accuracy: 0.5045\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4249 - accuracy: 0.8073 - val_loss: 0.9088 - val_accuracy: 0.5025\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4125 - accuracy: 0.8126 - val_loss: 0.9659 - val_accuracy: 0.5015\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4003 - accuracy: 0.8148 - val_loss: 0.8138 - val_accuracy: 0.5005\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3960 - accuracy: 0.8183 - val_loss: 0.8600 - val_accuracy: 0.5010\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3900 - accuracy: 0.8231 - val_loss: 0.9295 - val_accuracy: 0.5010\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3813 - accuracy: 0.8285 - val_loss: 0.9417 - val_accuracy: 0.5005\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3794 - accuracy: 0.8273 - val_loss: 0.9473 - val_accuracy: 0.5005\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3757 - accuracy: 0.8317 - val_loss: 0.8733 - val_accuracy: 0.3400\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3710 - accuracy: 0.8324 - val_loss: 1.0676 - val_accuracy: 0.5005\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3744 - accuracy: 0.8353 - val_loss: 0.9153 - val_accuracy: 0.4090\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3697 - accuracy: 0.8369 - val_loss: 1.0507 - val_accuracy: 0.4895\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3660 - accuracy: 0.8366 - val_loss: 1.3101 - val_accuracy: 0.2820\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3658 - accuracy: 0.8376 - val_loss: 0.9474 - val_accuracy: 0.3460\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3585 - accuracy: 0.8439 - val_loss: 1.0458 - val_accuracy: 0.4015\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3551 - accuracy: 0.8436 - val_loss: 0.9585 - val_accuracy: 0.3655\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3523 - accuracy: 0.8461 - val_loss: 1.2498 - val_accuracy: 0.3295\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3480 - accuracy: 0.8470 - val_loss: 0.7602 - val_accuracy: 0.5520\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3410 - accuracy: 0.8534 - val_loss: 0.7762 - val_accuracy: 0.5410\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3367 - accuracy: 0.8530 - val_loss: 1.0524 - val_accuracy: 0.3935\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3318 - accuracy: 0.8543 - val_loss: 0.7841 - val_accuracy: 0.5385\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3352 - accuracy: 0.8500 - val_loss: 1.0598 - val_accuracy: 0.4605\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3183 - accuracy: 0.8610 - val_loss: 1.4670 - val_accuracy: 0.4935\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3181 - accuracy: 0.8596 - val_loss: 0.7047 - val_accuracy: 0.6520\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3153 - accuracy: 0.8629 - val_loss: 0.9146 - val_accuracy: 0.5235\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3193 - accuracy: 0.8597 - val_loss: 0.9327 - val_accuracy: 0.5955\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3177 - accuracy: 0.8581 - val_loss: 0.7331 - val_accuracy: 0.5430\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3160 - accuracy: 0.8606 - val_loss: 0.8923 - val_accuracy: 0.4750\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3201 - accuracy: 0.8586 - val_loss: 1.2926 - val_accuracy: 0.4700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTJ5-GlCL-K4",
        "colab_type": "text"
      },
      "source": [
        "Based on the chart, the learning rate between 0.001(10e-3) and 0.0001(10e-4) appears to be the best option.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbttO97grwkF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d4342ad8-bd8b-4b24-df70-e7fb7e36af96"
      },
      "source": [
        "# review the learning rate performance\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hV9Z3v8fd379y4BEggAQyXRAw3oYpEvCveUafg1E6rjs/otNZjK2N72ukZnZmjc3TOdKbPHNuZDtbajm3taCnS1qLSMlZFq4gSFEUuwRBQQIFAALnm+j1/ZEG3MZedZCdrXz6v59lP9vqt31r7+2OTz15Ze13M3RERkfQVCbsAERHpWwp6EZE0p6AXEUlzCnoRkTSnoBcRSXMKehGRNJcVdgFtjRgxwktLS8MuQ0QkpaxevXqPuxe1Ny/pgr60tJTKysqwyxARSSlm9l5H8+LadWNmc8ysysyqzeyuduZ/x8zWBI9NZrY/Zt7NZvZu8Li5Z0MQEZGe6nKL3syiwALgcmA7sMrMlrj7+uN93P1/xvT/K2BG8LwQuBeoABxYHSy7L6GjEBGRDsWzRT8LqHb3GndvABYC8zrpfwPw8+D5lcCz7l4XhPuzwJzeFCwiIt0TT9CXANtiprcHbZ9gZuOBMuD57i4rIiJ9I9GHV14PLHb35u4sZGa3mVmlmVXW1tYmuCQRkcwWT9DvAMbGTI8J2tpzPX/cbRP3su7+sLtXuHtFUVG7RweJiEgPxXN45Sqg3MzKaA3p64Eb23Yys8lAAfBqTPMy4J/MrCCYvgK4u1cVd6C+qZmXNu3pi1XHzbrbP44FOutjHb1iHM0Ws+KPt7e//uPt1maB433M/th8fN0fbwt6B21mFvxsXceJ9QfTkQhEYvsEzyNmre0GkYgRMf44bUY0mH98+WjEYn5+fNwimaLLoHf3JjObT2toR4FH3H2dmd0HVLr7kqDr9cBCj7nAvbvXmdn9tH5YANzn7nWJHUKrQ8ea+NKjOv5eOheNtH4YRCNGVtTIihjRSIScqJGdFSErYuRkRVunoxFyslofuVkRBmRHyQseg3OzGJjb+nNIXjb5eVkMGZBN4aAcRgzKZciALH2oSNKwZLvxSEVFhffkhKnG5haqdh7sg4ri091/RqfrBTpbZ0ezOno//WN92p8T295ef3dvvx0/sYDHzDve24PZJ5YP5rl/cp4HHdyhxaHlRLufWGdLCzQHC7a40+zH+zvNLU6Lt/Zvbmmd19LiNAfLtLQ4TS1Oc0tL8NNpbG6hsdlpCn62TrfQ0NxCQ1ML9U0t1De2cKypmaMNzRxtbOZwfRMtnbw/2VGjOD+PkmEDOGlYHicXDWbiyMGUj8ynbPggIhF9CEhimdlqd69ob17SnRnbU9nRCNNKhoZdhmQId6e+qYWDx5o4eKyRg8eaOHC0kbrDDew93MCeQ/XsPHCMHfuPsmrrPp5c88GJZQsGZnPOhOGcO2EEV5w6kuL8vBBHIpkgbYJepD+Z2YndOEX5uV32P9LQRPXuQ2z88CCvbaljxeY9LF27k/ueWs+800/i1gtOZtKo/H6oXDJR2uy6EUkl7k717kM8+up7PLF6G8caW5h3+kn847XTyM/LDrs8SUGd7bpR0IuEbN/hBh55ZQsPLt/M2IIB/MeNZ2g3pHRbZ0Gv69GLhKxgUA7fuGISP//S2RxrbOEzD67gV29sD7ssSSMKepEkMauskGfuPJ8Z44Zx1y/Xsu6DA2GXJGlCQS+SRIYPzuX7N81k2MBsvrpwDUcbunU1EZF2KehFkkzhoBz+3+dOo3r3Ib712w1hlyNpQEEvkoQuKC/i1vPLePTV93huw66wy5EUp6AXSVLfnDOJyaPyuec362ju7DRckS4o6EWSVG5WlK9dVs6O/Uf5vbbqpRcU9CJJ7LIpIzlpaB4/XbE17FIkhSnoRZJYVjTCTeeMZ8XmvWzaFd5F+yS1KehFktz1Z44jJyuirXrpMQW9SJIrHJTDvNNO4ldv7ODA0cawy5EUpKAXSQE3n1vK0cZmnqjcFnYpkoIU9CIpYFrJUCrGF/D4a+93eHMZkY4o6EVSxLUzSqjZc5jNtYfCLkVSTFxBb2ZzzKzKzKrN7K4O+nzOzNab2TozezymvdnM1gSPJe0tKyJdu3RKMQC/37A75Eok1XR5hykziwILgMuB7cAqM1vi7utj+pQDdwPnufs+MyuOWcVRdz89wXWLZJzRQwdw6klD+P36Xdx+0YSwy5EUEs8W/Syg2t1r3L0BWAjMa9PnS8ACd98H4O7a5BDpA5dNGckb7+9j76H6sEuRFBJP0JcAsV/1bw/aYk0EJprZK2a20szmxMzLM7PKoP3a9l7AzG4L+lTW1tZ2awAimeSyKSNpcXihSr8nEr9EfRmbBZQDs4EbgB+a2bBg3vjg9lY3At81s0/8zenuD7t7hbtXFBUVJagkkfQzrWQII4fk6oqW0i3xBP0OYGzM9JigLdZ2YIm7N7r7FmATrcGPu+8IftYAy4EZvaxZJGOZGZdOGclLm2qpb9JNSSQ+8QT9KqDczMrMLAe4Hmh79MyTtG7NY2YjaN2VU2NmBWaWG9N+HrAeEemxy6YUc7ihmZU1dWGXIimiy6B39yZgPrAM2AAscvd1Znafmc0Nui0D9prZeuAF4JvuvheYAlSa2VtB+z/HHq0jIt137oQRDMiOaveNxM2S7Sy7iooKr6ysDLsMkaT2pUcrWbfjAK/cdQlmFnY5kgTMbHXwfegn6MxYkRR02ZRiPjhwjI07deli6ZqCXiQFXTyp9ZzE5zfqlBXpmoJeJAUVD8ljeslQBb3ERUEvkqIunlzMm+/vo+5wQ9ilSJJT0IukqEsnF9Pi8OImbdVL5xT0IilqeslQRgzO5fmNuhyCdE5BL5KiIhHj4klFvFi1m6bmlrDLkSSmoBdJYZdMLuajY02sfm9f2KVIElPQi6Sw88tHkB01HX0jnVLQi6Sw/LxsZpUV8pyCXjqhoBdJcRdPKqZ69yG21R0JuxRJUgp6kRR3yeTWs2RfqNJWvbRPQS+S4k4uGkzp8IG8oN030gEFvUgamD2pmBWb93K0QTcjkU9S0IukgYsnF1Pf1MLKmr1hlyJJSEEvkgbOKitkQHZU++mlXQp6kTSQlx3lvFOG8/zG3STbzYQkfHEFvZnNMbMqM6s2s7s66PM5M1tvZuvM7PGY9pvN7N3gcXOiCheRj5s9qZjt+46yufZQ2KVIksnqqoOZRYEFwOXAdmCVmS2JvfermZUDdwPnufs+MysO2guBe4EKwIHVwbI6X1skwS4+fpjlxlpOKc4PuRpJJvFs0c8Cqt29xt0bgIXAvDZ9vgQsOB7g7n58R+GVwLPuXhfMexaYk5jSRSRWybABTBqZr8shyCfEE/QlwLaY6e1BW6yJwEQze8XMVprZnG4si5ndZmaVZlZZW6tLror01OzJRazaWsdHxxrDLkWSSKK+jM0CyoHZwA3AD81sWLwLu/vD7l7h7hVFRUUJKkkk81w6eSRNLc7L7+4JuxRJIvEE/Q5gbMz0mKAt1nZgibs3uvsWYBOtwR/PsiKSIGeMG8bQAdn8fsOusEuRJBJP0K8Cys2szMxygOuBJW36PEnr1jxmNoLWXTk1wDLgCjMrMLMC4IqgTUT6QFY0wsWTilheVUtziw6zlFZdBr27NwHzaQ3oDcAid19nZveZ2dyg2zJgr5mtB14Avunue929Drif1g+LVcB9QZuI9JFLpoyk7nADa7btD7sUSRJdHl4J4O5LgaVt2u6Jee7A14NH22UfAR7pXZkiEq+LJhYRjRjPbdjFzPEFYZcjSUBnxoqkmaEDsjmztECHWcoJCnqRNHTp5JFs3HmQ7ft0MxJR0IukpUuntJ4lq616AQW9SFo6uWgwZSMG8dwGBb0o6EXS1qWTi3l1814O1zeFXYqETEEvkqauOHUUDc0tPLteJ09lOgW9SJqqGF/A2MIBLF69PexSJGQKepE0FYkY150xhlc272HH/qNhlyMhUtCLpLHrzhiDO/z6DW3VZzIFvUgaG1s4kLNPLmTx6u26xWAGU9CLpLnPzhzL1r1HWP2ebuyWqRT0ImnuqmmjGJgT1ZeyGUxBL5LmBuVmcfX00Tzz9occbWgOuxwJgYJeJANcd8YYDtY38dxGHVOfiRT0IhlgVlkhIwbn8tu1O8MuRUKgoBfJANGIMWfaSJ7fuFu7bzKQgl4kQ1w1bTRHG5t5cZMudJZp4gp6M5tjZlVmVm1md7Uz/xYzqzWzNcHj1ph5zTHtbe81KyL95KyyQgoGZvPbd7T7JtN0eStBM4sCC4DLge3AKjNb4u7r23T9hbvPb2cVR9399N6XKiK9kRWNcOWpo3j67Q851thMXnY07JKkn8SzRT8LqHb3GndvABYC8/q2LBHpC1dNH82h+iZefndP2KVIP4on6EuAbTHT24O2tq4zs7fNbLGZjY1pzzOzSjNbaWbXtvcCZnZb0KeytrY2/upFpFvOnTCcoQOyWfrOh2GXIv0oUV/GPgWUuvungGeBn8bMG+/uFcCNwHfNbELbhd39YXevcPeKoqKiBJUkIm1lRyNcPnUkz67fRUNTS9jlSD+JJ+h3ALFb6GOCthPcfa+71weTPwJmxszbEfysAZYDM3pRr4j00lXTRnHwWBOvVGv3TaaIJ+hXAeVmVmZmOcD1wMeOnjGz0TGTc4ENQXuBmeUGz0cA5wFtv8QVkX50fvkI8nOzWLpWu28yRZdH3bh7k5nNB5YBUeARd19nZvcBle6+BLjTzOYCTUAdcEuw+BTgB2bWQuuHyj+3c7SOiPSj3Kwol00dyX+v38U/NbeQHdXpNOmuy6AHcPelwNI2bffEPL8buLud5VYA03tZo4gk2NXTR/PrN3ewYvNeLpqo78XSnT7KRTLQBeUjGJybxdK3tfsmEyjoRTJQXnaUS6cUs2z9ThqbdfRNulPQi2Soq6ePZv+RRlbW7A27FOljCnqRDHXRxCIG5UR19E0GUNCLZKi87CiXTBnJsnW7aNLum7SmoBfJYNdMH0Xd4QZe21IXdinShxT0IhnsoonF5GZFeHa9bjGYzhT0IhlsQE6UcyYMZ3mVbkaSzhT0IhnuksnFbN17hC17DoddivQRBb1Ihrt4UjEAz2/UVn26UtCLZLixhQM5pXiwdt+kMQW9iHDxpCJeq6njcH1T2KVIH1DQiwgXTyqmoblF16hPUwp6EaGitJDBuVm8UKVbeaYjBb2IkJMV4fxTRrC8ajfuHnY5kmAKehEBWg+z/PDAMTbuPBh2KZJgCnoRAWD2pNYbkLy4Sbtv0k1cQW9mc8ysysyqzeyudubfYma1ZrYmeNwaM+9mM3s3eNycyOJFJHGKh+Rx8ohBVG7dF3YpkmBd3krQzKLAAuByYDuwysyWtHPv11+4+/w2yxYC9wIVgAOrg2X1P0kkCc0cX8BzG1v305tZ2OVIgsSzRT8LqHb3GndvABYC8+Jc/5XAs+5eF4T7s8CcnpUqIn1t5vgC6g436HIIaSaeoC8BtsVMbw/a2rrOzN42s8VmNrY7y5rZbWZWaWaVtbXaPygSlpnjCwBY/Z7+6E4nifoy9img1N0/RetW+0+7s7C7P+zuFe5eUVSkO9KLhGVC0WCG5GXxxvsK+nQST9DvAMbGTI8J2k5w973uXh9M/giYGe+yIpI8IhHjjPEF2qJPM/EE/Sqg3MzKzCwHuB5YEtvBzEbHTM4FNgTPlwFXmFmBmRUAVwRtIpKkZo4rYNOuQxw42hh2KZIgXQa9uzcB82kN6A3AIndfZ2b3mdncoNudZrbOzN4C7gRuCZatA+6n9cNiFXBf0CYiSer4fvo3tfsmbXR5eCWAuy8FlrZpuyfm+d3A3R0s+wjwSC9qFJF+dNrYYUQjxur39jE7uFa9pDadGSsiHzMoN4spo/O1nz6NKOhF5BNmjitgzbb9NDW3hF2KJICCXkQ+4YzxBRxpaNYFztKEgl5EPkEnTqUXBb2IfELJsAGMGpJHpYI+LSjoReQTzIxZZYW8vmWvbkSSBhT0ItKuM8sK2fVRPe/XHQm7FOklBb2ItOusskIAXtuicxxTnYJeRNp1StFgCgZms0pBn/IU9CLSrkjEOLO0kNe3KuhTnYJeRDo0q6yQ9/YeYeeBY2GXIr2goBeRDs0K9tNrqz61KehFpENTRw9hUE6U17fsDbsU6QUFvYh0KCsaYWZpIau26MSpVKagF5FOzSotoGrXQfYdbgi7FOkhBb2IdGpW2XAAVmk/fcpS0ItIpz41Zig5WRFW1ijoU1VcQW9mc8ysysyqzeyuTvpdZ2ZuZhXBdKmZHTWzNcHjoUQVLiL9Iy87ytknD+fZDTt13ZsU1WXQm1kUWABcBUwFbjCzqe30ywe+CrzWZtZmdz89eNyegJpFpJ9dM30U2+qO8s6Oj8IuRXogni36WUC1u9e4ewOwEJjXTr/7gX8BdGaFSJq5YuooohHjmbUfhl2K9EA8QV8CbIuZ3h60nWBmZwBj3f2ZdpYvM7M3zexFM7ug56WKSFgKBuVw3ikjWLr2Q+2+SUG9/jLWzCLAA8A32pn9ITDO3WcAXwceN7Mh7azjNjOrNLPK2tra3pYkIn3gmumjeL/uCOs+0O6bVBNP0O8AxsZMjwnajssHpgHLzWwrcDawxMwq3L3e3fcCuPtqYDMwse0LuPvD7l7h7hVFRUU9G4mI9Knju2+eflu7b1JNPEG/Cig3szIzywGuB5Ycn+nuB9x9hLuXunspsBKY6+6VZlYUfJmLmZ0MlAM1CR+FiPS5gkE5nDthuHbfpKAug97dm4D5wDJgA7DI3deZ2X1mNreLxS8E3jazNcBi4HZ318G4IinqTz41WrtvUlBWPJ3cfSmwtE3bPR30nR3z/JfAL3tRn4gkkSumjuJvf/0Oz6z9kGklQ8MuR+KkM2NFJG4Fg3K4sHwEv35jB03NLWGXI3FS0ItIt1w/axw7PzrG8xt3h12KxElBLyLdcunkYkYOyeXx198PuxSJk4JeRLolKxrh8xVjeXFTLdvqjoRdjsRBQS8i3fb5WeMw4BertnXZV8KnoBeRbisZNoDZk4r5ReU2GvWlbNJT0ItIj9w4axy1B+t5bsOusEuRLijoRaRHZk8qYvTQPH628r2wS5EuKOhFpEeyohH+4pxSXqney9rtB8IuRzqhoBeRHrvp7HHk52Xx4PLqsEuRTijoRaTH8vOyufmcUn63bifVuw+FXY50QEEvIr3yl+eVkpsV4aEXN4ddinRAQS8ivTJ8cC7XnzmOJ9/cwY79R8MuR9qhoBeRXvvShScD8MOXdLuJZKSgF5FeKxk2gGtnlLBw1fvsOVQfdjnShoJeRBLiy7MnUN/Uwn++vCXsUqQNBb2IJMSEosFcM300P3v1PQ4caQy7HIkRV9Cb2RwzqzKzajO7q5N+15mZm1lFTNvdwXJVZnZlIooWkeR0x8WncKi+iZ+s2Bp2KRKjy6APbu69ALgKmArcYGZT2+mXD3wVeC2mbSqtNxM/FZgDPHj8ZuEikn6mjB7CZVOK+fGKLRyubwq7HAnEs0U/C6h29xp3bwAWAvPa6Xc/8C/AsZi2ecBCd6939y1AdbA+EUlTd1x8CvuPNPLYa7oGTrKIJ+hLgNiLTm8P2k4wszOAse7+THeXFZH0MmNcAeefMoLvPVfNb9d+GHY5QgK+jDWzCPAA8I1erOM2M6s0s8ra2treliQiIfvn66ZzctEgvvzYG/zdr9dyrLE57JIyWjxBvwMYGzM9Jmg7Lh+YBiw3s63A2cCS4AvZrpYFwN0fdvcKd68oKirq3ghEJOmMKRjIE7efy20Xnsxjr73Pnz64gr06vj408QT9KqDczMrMLIfWL1eXHJ/p7gfcfYS7l7p7KbASmOvulUG/680s18zKgHLg9YSPQkSSTk5WhL+9egqP3FJBTe0hbvrP19l/pCHssjJSl0Hv7k3AfGAZsAFY5O7rzOw+M5vbxbLrgEXAeuB3wB3urr/hRDLIJZNH8sO/qGDz7kPc/MjrfHRMx9j3N3P3sGv4mIqKCq+srAy7DBFJsN+v38Xt/7WaGeOG8ditZ5OTpfM1E8nMVrt7RXvz9C8tIv3isqkj+c7nT2fV1n38+3Pvhl1ORlHQi0i/+fRpJ/HZmWP4/oubWbNtf9jlZAwFvYj0q3s+PZWR+bl8Y9EaHXbZTxT0ItKvhuRl8y+f/RSbaw/zr8uqwi4nI2SFXYCIZJ4Lyov487PG8aOXt/By9R6umjaaaz41ilOK88MuLS1pi15EQnHPp6dy76enkp+XxXef28RlD7zEXz/xFnWHdax9ounwShEJ3a6PjvGTFVv54Us15Odl8bdXT+GzM8dgZmGXljJ0eKWIJLWRQ/L4mzmTeebOC5hQNJhvLn6b+T9/82OXOt5/pIEHl1dTtfNgiJWmJm3Ri0hSaWlxHv5DDd/+3UbKi/NZ8Odn8GrNXh747yr2HWlkYE6Uf/2z07h6+uiwS00q2qIXkZQRiRi3XzSBR79wFrsPHuOyB17kfz/5DpNG5fNfXzyLyaPy+cpjb/Dt322kuSW5NlSTlbboRSRpbas7wnee3cTlU0cyZ9oozIz6pmbu/c06Fq7axpmlBXzrM9N1tA6db9Er6EUk5bg7v3xjB/c/vZ6jDc3ccfEpXDezhJysCDnRCEMHZGfcF7kKehFJS7UH67nv6fU89dYHH2u/fOpIHrppJtFI5oR9Z0GvE6ZEJGUV5efyvRtm8BfnjGdL7WEamlvYXHuIH7+ylQeereKbV04Ou8SkoKAXkZR3ZmkhZ5YWAq27dY42NLPghc1MO2koV+noHB11IyLpxcz4P/NOZca4YXzjibfYtEvH3SvoRSTt5GZFeeimmQzKzeK6B1fw8EubaWhqCbus0CjoRSQtjRySxxP/4xzOLCvkn5Zu5MrvvsTv3tlJSwYeex9X0JvZHDOrMrNqM7urnfm3m9laM1tjZi+b2dSgvdTMjgbta8zsoUQPQESkI6UjBvHILWfy4788EzO4/b9Wc9W//YHfrNlBU3PmbOF3eXilmUWBTcDlwHZgFXCDu6+P6TPE3T8Kns8FvuLuc8ysFHja3afFW5AOrxSRvtDU3MJTb3/Agy9s5t3dhygvHsy/3zCDKaOHhF1aQvT2EgizgGp3r3H3BmAhMC+2w/GQDwwCMu9vIxFJalnRCH86YwzLvnYhD910BgeONjJvwSs8+upWku18okSLJ+hLgG0x09uDto8xszvMbDPwbeDOmFllZvammb1oZhe09wJmdpuZVZpZZW1tbTfKFxHpnkjEmDNtNL/96gWcN2E49/xmHXc8/gaNabwrJ2Ffxrr7AnefAPwN8PdB84fAOHefAXwdeNzMPvF3krs/7O4V7l5RVFSUqJJERDo0fHAu/3nzmdx11WSWrt3JPyxZl7Zb9vGcMLUDGBszPSZo68hC4PsA7l4P1AfPVwdb/BMB7YQXkdAdv1Lm/iONPPTiZiYUDeYL55eFXVbCxbNFvwooN7MyM8sBrgeWxHYws/KYyWuAd4P2ouDLXMzsZKAcqElE4SIiifK/rpzElaeO5B+fWc/zG3eFXU7CdRn07t4EzAeWARuARe6+zszuC46wAZhvZuvMbA2tu2huDtovBN4O2hcDt7t7XcJHISLSC5GI8Z3Pn87Uk4bwlcfeYPHq7WGXlFC6eqWISKD2YD1/9fM3WFlTx2dmlHD/tdMYlJsalwTTHaZEROJQlJ/LY7eezdcuK+fJNTv49Pdepnr3obDL6jUFvYhIjGjE+NplE3ns1rP56Fgjf7rgFV6o2g3AvsMN3Pubd5j+D8t4Z8eBkCuNX2r8TSIi0s/OmTCc38w/n1t/WskXf7KKz84cw7J1uzh4rJGIGYsqtzGtZGjYZcZFW/QiIh0oGTaAX375HK48dRSLKrczvWQov/3qhVxx6kiWrv0wZa6Xoy16EZFODMzJYsGNZ/Be3RFKhw/EzJh72kksXbuTFZv3cuHE5D/JU1v0IiJdiESMshGDTtxwfPakYvJzs1jS5l61yUpBLyLSTXnZUa44dRTL3tnJscbmsMvpkoJeRKQH5p5+Egfrm1helfwXYlTQi4j0wHkThjN8UA5PvZ2Y3TcHjjZSU9s3x+zry1gRkR7Iika4evponli9jQ0ffsTg3CwG5kQpHJRzYl8+wP4jDTy4fDNmcO6EEZxZWsDAnI9H78qavXxj0Vvk52Wx9M4LiESs7cv1rtaErk1EJINcO+MkfrbyPa76tz+caJs5voCvzJ7AJZOLeW7Dbu7+9VrqDjcQMfjBizVkR42K8YVcOqWYiyYW8cs3dvCDlzYzvnAg3/rM9ISHPOhaNyIivbJi8x72HGqgvrGZ2kP1PLbyfXbsP8pJQ/P44MAxJo/K51//7DROLhpE5dZ9vFK9h+VVtVTtOnhiHTfMGsvfXzO1V9fV6exaNwp6EZEEamxu4am3PuCJyu3MKivkjotPISfrk1+Hbt93hBc31TKucCAXlPf+WHwFvYhImtPVK0VEMpiCXkQkzSnoRUTSXFxBb2ZzzKzKzKrN7K525t9uZmvNbI2ZvWxmU2Pm3R0sV2VmVyayeBER6VqXQR/c3HsBcBUwFbghNsgDj7v7dHc/Hfg28ECw7FRabyZ+KjAHePD4zcJFRKR/xLNFPwuodvcad28AFgLzYju4+0cxk4OA44fyzAMWunu9u28BqoP1iYhIP4nn6PwSYFvM9HbgrLadzOwO4OtADnBJzLIr2yxb0s6ytwG3AYwbNy6eukVEJE4J+zLW3Re4+wTgb4C/7+ayD7t7hbtXFBUl/0X8RURSSTxb9DuAsTHTY4K2jiwEvt/DZVm9evUeM3sPGArE3n23s+nY5yOAPZ29Rpzavl5v+nY0v732eMfZF2PuqKae9EvUmNtOp8t73dW/g95rvdfdHfP4Due4e6cPWj8MaoAyWnfLvAWc2qZPeczzTwOVwfNTg/65wfI1QLSr1wyWfTje6TbPK+NZf3dfvzd9O5rfXns3xpnwMXdn3P015nR9r7v6d9B7rfc6ke91l1v07t5kZvOBZUAUeMTd15nZfUEhS4D5ZnYZ0AjsA24Oll1nZouA9UATcIe7x4LXwscAAANTSURBVHs7lqe6Md12XiJ0Z51d9e1ofnvt8Y6zL8bcnfX215jbTqfLe93Vv4Pea73XCZN017rpLTOr9A6u95CuMnHMkJnjzsQxQ2aOO5FjTsczYx8Ou4AQZOKYITPHnYljhswcd8LGnHZb9CIi8nHpuEUvIiIxFPQiImlOQS8ikuYyJujNbJyZPWlmj7R3Bc50ZWYXmNlDZvYjM1sRdj39wcwiZvZ/zex7ZnZz2PX0FzObbWZ/CN7v2WHX01/MbJCZVZrZn4RdS38xsynB+7zYzL7cVf+UCPognHeb2Ttt2ju9fHIb04HF7v4FYEafFZtAiRi3u//B3W8HngZ+2pf1JkKC3ut5tJ6F3Ujr9ZWSXoLG7cAhII8UGHeCxgytl11Z1DdVJl6Cfq83BL/XnwPO6/I1U+GoGzO7kNb/wI+6+7SgLQpsAi6n9T/1KuAGWk/q+labVXwBaAYW0/rL8DN3/3H/VN9ziRi3u+8OllsEfNHdD5LEEvRefwHY5+4/MLPF7v7Z/qq/pxI07j3u3mJmI4EH3P3P+6v+nkjQmE8DhtP64bbH3Z/un+p7LlG/12Y2F/gyrXn2eGevGc+1bkLn7i+ZWWmb5hOXTwYws4XAPHf/FvCJP+HM7K+Be4N1LQaSPugTMe6gzzjgQLKHPCTsvd4ONAST8Z6JHapEvdeBfbRediSpJei9nk3rpdGnAkfNbKm7t/Rl3b2VqPc6uCrBEjN7Bkj9oO9AXJdPjvE74B/M7EZgax/W1de6O26AL5ICH2yd6O6YfwV8z8wuAF7qy8L6WLfGbWafAa4EhgH/0bel9Zlujdnd/w7AzG4h+IumT6vrO919r2cDn6H1A31pVytP5aDvFnd/B0j6P+H7grvfG3YN/cndj9D64ZZR3P1XtH7IZRx3/0nYNfQnd18OLI+3f0p8GduBbl8COU1k4rgzccyQmePOxDFDH487lYN+FVBuZmVmlkPrvWmXhFxTf8jEcWfimCEzx52JY4a+Hneirnfclw/g58CH/PFwuS8G7VfT+k31ZuDvwq5T49aYNW6NORnHnRKHV4qISM+l8q4bERGJg4JeRCTNKehFRNKcgl5EJM0p6EVE0pyCXkQkzSnoRUTSnIJeRCTNKehFRNLc/weKBF5nPCNHRgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U74NtimIMpWC",
        "colab_type": "text"
      },
      "source": [
        "After multiple iterations of parameter tuning the embedding and the model, the best performing model uses a learning rate of 0.00001 (10e-5) and the settings below.  \n",
        "\n",
        "The validation accuracy was 0.7335 at epoch 19 with a training accuracy of 0.7559.  The validationa accuracy starts to increase and become unstable after epoch 20.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yAmjJqEyCOF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc737fc4-f05b-44ea-8440-3efa90e49283"
      },
      "source": [
        "\n",
        "vocab_size = 1000\n",
        "embedding_dim = 32\n",
        "max_length = 50\n",
        "model_number = 1\n",
        "epoch = 100\n",
        "learning_rate = 0.00001\n",
        "lr_cb = False\n",
        "cp_cb = True\n",
        "\n",
        "tokenizer, model_lr_sched, callback_list = define_model(vocab_size, embedding_dim, max_length, model_number, epoch, learning_rate , lr_cb, cp_cb)\n",
        "\n",
        "history = model_lr_sched.fit(padded, train_y, \n",
        "                  epochs=epoch, \n",
        "                  validation_data = (testing_padded, test_y),\n",
        "                  callbacks=[callback_list],\n",
        "                  verbose = 1)\n",
        "\n",
        "# review the learning rate performance\n",
        "chart_acc_loss(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 32)            32000     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               164864    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 229,889\n",
            "Trainable params: 229,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5253\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50000, saving model to 1_weights-improvement-01-0.50.hdf5\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.6930 - accuracy: 0.5250 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "244/250 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.5797\n",
            "Epoch 00002: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.6925 - accuracy: 0.5801 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.5996\n",
            "Epoch 00003: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.6919 - accuracy: 0.5996 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.6137\n",
            "Epoch 00004: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6909 - accuracy: 0.6137 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.6339\n",
            "Epoch 00005: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6893 - accuracy: 0.6345 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 6/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.6519\n",
            "Epoch 00006: val_accuracy did not improve from 0.50000\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6864 - accuracy: 0.6522 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.6714\n",
            "Epoch 00007: val_accuracy improved from 0.50000 to 0.66250, saving model to 1_weights-improvement-07-0.66.hdf5\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6810 - accuracy: 0.6714 - val_loss: 0.6916 - val_accuracy: 0.6625\n",
            "Epoch 8/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.6712 - accuracy: 0.6798\n",
            "Epoch 00008: val_accuracy improved from 0.66250 to 0.68400, saving model to 1_weights-improvement-08-0.68.hdf5\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6711 - accuracy: 0.6799 - val_loss: 0.6748 - val_accuracy: 0.6840\n",
            "Epoch 9/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.6549 - accuracy: 0.6791\n",
            "Epoch 00009: val_accuracy improved from 0.68400 to 0.70450, saving model to 1_weights-improvement-09-0.70.hdf5\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6549 - accuracy: 0.6793 - val_loss: 0.6132 - val_accuracy: 0.7045\n",
            "Epoch 10/100\n",
            "244/250 [============================>.] - ETA: 0s - loss: 0.6368 - accuracy: 0.6902\n",
            "Epoch 00010: val_accuracy did not improve from 0.70450\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.6366 - accuracy: 0.6899 - val_loss: 0.5701 - val_accuracy: 0.6920\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.6981\n",
            "Epoch 00011: val_accuracy improved from 0.70450 to 0.71250, saving model to 1_weights-improvement-11-0.71.hdf5\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6199 - accuracy: 0.6981 - val_loss: 0.5811 - val_accuracy: 0.7125\n",
            "Epoch 12/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.6070 - accuracy: 0.7078\n",
            "Epoch 00012: val_accuracy did not improve from 0.71250\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6068 - accuracy: 0.7084 - val_loss: 0.5921 - val_accuracy: 0.7000\n",
            "Epoch 13/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.5937 - accuracy: 0.7176\n",
            "Epoch 00013: val_accuracy did not improve from 0.71250\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5933 - accuracy: 0.7176 - val_loss: 0.6356 - val_accuracy: 0.7020\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5861 - accuracy: 0.7228\n",
            "Epoch 00014: val_accuracy did not improve from 0.71250\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5861 - accuracy: 0.7228 - val_loss: 0.6264 - val_accuracy: 0.7065\n",
            "Epoch 15/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.5761 - accuracy: 0.7235\n",
            "Epoch 00015: val_accuracy improved from 0.71250 to 0.71800, saving model to 1_weights-improvement-15-0.72.hdf5\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5763 - accuracy: 0.7239 - val_loss: 0.6256 - val_accuracy: 0.7180\n",
            "Epoch 16/100\n",
            "244/250 [============================>.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7413\n",
            "Epoch 00016: val_accuracy did not improve from 0.71800\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5669 - accuracy: 0.7427 - val_loss: 0.6398 - val_accuracy: 0.7100\n",
            "Epoch 17/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.5592 - accuracy: 0.7448\n",
            "Epoch 00017: val_accuracy improved from 0.71800 to 0.72950, saving model to 1_weights-improvement-17-0.73.hdf5\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5590 - accuracy: 0.7446 - val_loss: 0.6350 - val_accuracy: 0.7295\n",
            "Epoch 18/100\n",
            "244/250 [============================>.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7468\n",
            "Epoch 00018: val_accuracy did not improve from 0.72950\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5512 - accuracy: 0.7465 - val_loss: 0.6349 - val_accuracy: 0.7280\n",
            "Epoch 19/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.5427 - accuracy: 0.7559\n",
            "Epoch 00019: val_accuracy improved from 0.72950 to 0.73350, saving model to 1_weights-improvement-19-0.73.hdf5\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5428 - accuracy: 0.7558 - val_loss: 0.6520 - val_accuracy: 0.7335\n",
            "Epoch 20/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7597\n",
            "Epoch 00020: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5353 - accuracy: 0.7594 - val_loss: 0.6404 - val_accuracy: 0.7280\n",
            "Epoch 21/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.5289 - accuracy: 0.7559\n",
            "Epoch 00021: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5286 - accuracy: 0.7560 - val_loss: 0.6148 - val_accuracy: 0.7250\n",
            "Epoch 22/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.5198 - accuracy: 0.7651\n",
            "Epoch 00022: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5196 - accuracy: 0.7654 - val_loss: 0.6186 - val_accuracy: 0.7205\n",
            "Epoch 23/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7692\n",
            "Epoch 00023: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5125 - accuracy: 0.7689 - val_loss: 0.6135 - val_accuracy: 0.7165\n",
            "Epoch 24/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.7726\n",
            "Epoch 00024: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.5046 - accuracy: 0.7725 - val_loss: 0.6086 - val_accuracy: 0.7090\n",
            "Epoch 25/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.4959 - accuracy: 0.7754\n",
            "Epoch 00025: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4955 - accuracy: 0.7755 - val_loss: 0.6442 - val_accuracy: 0.6895\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4867 - accuracy: 0.7739\n",
            "Epoch 00026: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4867 - accuracy: 0.7739 - val_loss: 0.6024 - val_accuracy: 0.6990\n",
            "Epoch 27/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.7849\n",
            "Epoch 00027: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4749 - accuracy: 0.7844 - val_loss: 0.6928 - val_accuracy: 0.6590\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.7905\n",
            "Epoch 00028: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4636 - accuracy: 0.7905 - val_loss: 0.7100 - val_accuracy: 0.6480\n",
            "Epoch 29/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.7928\n",
            "Epoch 00029: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4556 - accuracy: 0.7921 - val_loss: 0.6849 - val_accuracy: 0.6530\n",
            "Epoch 30/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.4487 - accuracy: 0.7964\n",
            "Epoch 00030: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4484 - accuracy: 0.7964 - val_loss: 0.7116 - val_accuracy: 0.6325\n",
            "Epoch 31/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.4438 - accuracy: 0.7958\n",
            "Epoch 00031: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4435 - accuracy: 0.7959 - val_loss: 0.6971 - val_accuracy: 0.6430\n",
            "Epoch 32/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.4385 - accuracy: 0.7982\n",
            "Epoch 00032: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4383 - accuracy: 0.7987 - val_loss: 0.7030 - val_accuracy: 0.6345\n",
            "Epoch 33/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.8032\n",
            "Epoch 00033: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4355 - accuracy: 0.8030 - val_loss: 0.8289 - val_accuracy: 0.5735\n",
            "Epoch 34/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.8047\n",
            "Epoch 00034: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4314 - accuracy: 0.8049 - val_loss: 0.7305 - val_accuracy: 0.6110\n",
            "Epoch 35/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.4254 - accuracy: 0.8110\n",
            "Epoch 00035: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4253 - accuracy: 0.8110 - val_loss: 0.8669 - val_accuracy: 0.5610\n",
            "Epoch 36/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8122\n",
            "Epoch 00036: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4240 - accuracy: 0.8109 - val_loss: 0.8667 - val_accuracy: 0.5575\n",
            "Epoch 37/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.4185 - accuracy: 0.8124\n",
            "Epoch 00037: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4190 - accuracy: 0.8125 - val_loss: 0.7505 - val_accuracy: 0.5960\n",
            "Epoch 38/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.4161 - accuracy: 0.8134\n",
            "Epoch 00038: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4163 - accuracy: 0.8127 - val_loss: 0.8694 - val_accuracy: 0.5515\n",
            "Epoch 39/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8151\n",
            "Epoch 00039: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4139 - accuracy: 0.8150 - val_loss: 0.8642 - val_accuracy: 0.5550\n",
            "Epoch 40/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.4088 - accuracy: 0.8183\n",
            "Epoch 00040: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4098 - accuracy: 0.8181 - val_loss: 0.7965 - val_accuracy: 0.5805\n",
            "Epoch 41/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.4066 - accuracy: 0.8160\n",
            "Epoch 00041: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4066 - accuracy: 0.8163 - val_loss: 0.7425 - val_accuracy: 0.5935\n",
            "Epoch 42/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8204\n",
            "Epoch 00042: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4053 - accuracy: 0.8199 - val_loss: 0.8511 - val_accuracy: 0.5485\n",
            "Epoch 43/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.4011 - accuracy: 0.8206\n",
            "Epoch 00043: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4010 - accuracy: 0.8207 - val_loss: 0.7760 - val_accuracy: 0.5735\n",
            "Epoch 44/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8189\n",
            "Epoch 00044: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3997 - accuracy: 0.8194 - val_loss: 0.8166 - val_accuracy: 0.5605\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8230\n",
            "Epoch 00045: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3974 - accuracy: 0.8230 - val_loss: 0.7078 - val_accuracy: 0.6050\n",
            "Epoch 46/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3977 - accuracy: 0.8213\n",
            "Epoch 00046: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3976 - accuracy: 0.8213 - val_loss: 0.7970 - val_accuracy: 0.5595\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.3918 - accuracy: 0.8251\n",
            "Epoch 00047: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3918 - accuracy: 0.8251 - val_loss: 0.6622 - val_accuracy: 0.6385\n",
            "Epoch 48/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3918 - accuracy: 0.8213\n",
            "Epoch 00048: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3922 - accuracy: 0.8216 - val_loss: 0.8033 - val_accuracy: 0.5635\n",
            "Epoch 49/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3917 - accuracy: 0.8248\n",
            "Epoch 00049: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3905 - accuracy: 0.8254 - val_loss: 0.7876 - val_accuracy: 0.5580\n",
            "Epoch 50/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8247\n",
            "Epoch 00050: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3865 - accuracy: 0.8248 - val_loss: 0.7959 - val_accuracy: 0.5605\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.3867 - accuracy: 0.8257\n",
            "Epoch 00051: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3867 - accuracy: 0.8257 - val_loss: 0.8612 - val_accuracy: 0.5370\n",
            "Epoch 52/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8281\n",
            "Epoch 00052: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3835 - accuracy: 0.8290 - val_loss: 0.8089 - val_accuracy: 0.5560\n",
            "Epoch 53/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3846 - accuracy: 0.8293\n",
            "Epoch 00053: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3831 - accuracy: 0.8300 - val_loss: 0.6804 - val_accuracy: 0.6080\n",
            "Epoch 54/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3832 - accuracy: 0.8271\n",
            "Epoch 00054: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3824 - accuracy: 0.8275 - val_loss: 0.9016 - val_accuracy: 0.5350\n",
            "Epoch 55/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3806 - accuracy: 0.8313\n",
            "Epoch 00055: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3794 - accuracy: 0.8315 - val_loss: 0.7233 - val_accuracy: 0.5930\n",
            "Epoch 56/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.8302\n",
            "Epoch 00056: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3783 - accuracy: 0.8307 - val_loss: 0.7567 - val_accuracy: 0.5845\n",
            "Epoch 57/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8318\n",
            "Epoch 00057: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3776 - accuracy: 0.8311 - val_loss: 0.7386 - val_accuracy: 0.5790\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8296\n",
            "Epoch 00058: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3775 - accuracy: 0.8296 - val_loss: 0.8780 - val_accuracy: 0.5320\n",
            "Epoch 59/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8302\n",
            "Epoch 00059: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3753 - accuracy: 0.8294 - val_loss: 0.6954 - val_accuracy: 0.6040\n",
            "Epoch 60/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3734 - accuracy: 0.8331\n",
            "Epoch 00060: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3739 - accuracy: 0.8326 - val_loss: 0.7665 - val_accuracy: 0.5655\n",
            "Epoch 61/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3721 - accuracy: 0.8322\n",
            "Epoch 00061: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3730 - accuracy: 0.8313 - val_loss: 0.8930 - val_accuracy: 0.5385\n",
            "Epoch 62/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.3725 - accuracy: 0.8340\n",
            "Epoch 00062: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3732 - accuracy: 0.8335 - val_loss: 0.8033 - val_accuracy: 0.5550\n",
            "Epoch 63/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3725 - accuracy: 0.8356\n",
            "Epoch 00063: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3720 - accuracy: 0.8356 - val_loss: 0.8426 - val_accuracy: 0.5375\n",
            "Epoch 64/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8355\n",
            "Epoch 00064: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3714 - accuracy: 0.8350 - val_loss: 0.8733 - val_accuracy: 0.5330\n",
            "Epoch 65/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8373\n",
            "Epoch 00065: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3684 - accuracy: 0.8371 - val_loss: 0.7659 - val_accuracy: 0.5740\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8346\n",
            "Epoch 00066: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3689 - accuracy: 0.8346 - val_loss: 0.7949 - val_accuracy: 0.5570\n",
            "Epoch 67/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8352\n",
            "Epoch 00067: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3667 - accuracy: 0.8350 - val_loss: 0.7886 - val_accuracy: 0.5590\n",
            "Epoch 68/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8381\n",
            "Epoch 00068: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3661 - accuracy: 0.8381 - val_loss: 0.7410 - val_accuracy: 0.5735\n",
            "Epoch 69/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8353\n",
            "Epoch 00069: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3657 - accuracy: 0.8356 - val_loss: 0.8096 - val_accuracy: 0.5510\n",
            "Epoch 70/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8370\n",
            "Epoch 00070: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3651 - accuracy: 0.8372 - val_loss: 0.7988 - val_accuracy: 0.5560\n",
            "Epoch 71/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.3669 - accuracy: 0.8351\n",
            "Epoch 00071: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3663 - accuracy: 0.8356 - val_loss: 0.8511 - val_accuracy: 0.5370\n",
            "Epoch 72/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8383\n",
            "Epoch 00072: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3626 - accuracy: 0.8389 - val_loss: 0.7620 - val_accuracy: 0.5625\n",
            "Epoch 73/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3612 - accuracy: 0.8387\n",
            "Epoch 00073: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3616 - accuracy: 0.8388 - val_loss: 0.7603 - val_accuracy: 0.5580\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.8374\n",
            "Epoch 00074: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3622 - accuracy: 0.8374 - val_loss: 0.7264 - val_accuracy: 0.5835\n",
            "Epoch 75/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8397\n",
            "Epoch 00075: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3601 - accuracy: 0.8400 - val_loss: 0.8621 - val_accuracy: 0.5330\n",
            "Epoch 76/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8391\n",
            "Epoch 00076: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3618 - accuracy: 0.8393 - val_loss: 0.7870 - val_accuracy: 0.5490\n",
            "Epoch 77/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8405\n",
            "Epoch 00077: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3593 - accuracy: 0.8406 - val_loss: 0.7985 - val_accuracy: 0.5535\n",
            "Epoch 78/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.8425\n",
            "Epoch 00078: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3570 - accuracy: 0.8425 - val_loss: 0.8149 - val_accuracy: 0.5435\n",
            "Epoch 79/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8430\n",
            "Epoch 00079: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3585 - accuracy: 0.8430 - val_loss: 0.7024 - val_accuracy: 0.5865\n",
            "Epoch 80/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3571 - accuracy: 0.8400\n",
            "Epoch 00080: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3569 - accuracy: 0.8401 - val_loss: 0.7187 - val_accuracy: 0.5855\n",
            "Epoch 81/100\n",
            "244/250 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8439\n",
            "Epoch 00081: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3574 - accuracy: 0.8430 - val_loss: 0.7486 - val_accuracy: 0.5660\n",
            "Epoch 82/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3573 - accuracy: 0.8394\n",
            "Epoch 00082: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3572 - accuracy: 0.8395 - val_loss: 0.7691 - val_accuracy: 0.5465\n",
            "Epoch 83/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3581 - accuracy: 0.8408\n",
            "Epoch 00083: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3577 - accuracy: 0.8416 - val_loss: 0.8477 - val_accuracy: 0.5325\n",
            "Epoch 84/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.3561 - accuracy: 0.8453\n",
            "Epoch 00084: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3556 - accuracy: 0.8455 - val_loss: 0.7264 - val_accuracy: 0.5775\n",
            "Epoch 85/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8415\n",
            "Epoch 00085: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3549 - accuracy: 0.8418 - val_loss: 0.6973 - val_accuracy: 0.5770\n",
            "Epoch 86/100\n",
            "247/250 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8439\n",
            "Epoch 00086: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3524 - accuracy: 0.8443 - val_loss: 0.8301 - val_accuracy: 0.5335\n",
            "Epoch 87/100\n",
            "248/250 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8431\n",
            "Epoch 00087: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3546 - accuracy: 0.8432 - val_loss: 0.7518 - val_accuracy: 0.5590\n",
            "Epoch 88/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.8451\n",
            "Epoch 00088: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3529 - accuracy: 0.8451 - val_loss: 0.6774 - val_accuracy: 0.6020\n",
            "Epoch 89/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.8445\n",
            "Epoch 00089: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3506 - accuracy: 0.8447 - val_loss: 0.9520 - val_accuracy: 0.5140\n",
            "Epoch 90/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3524 - accuracy: 0.8445\n",
            "Epoch 00090: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3527 - accuracy: 0.8446 - val_loss: 0.8399 - val_accuracy: 0.5260\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.8453\n",
            "Epoch 00091: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3502 - accuracy: 0.8453 - val_loss: 0.8662 - val_accuracy: 0.5235\n",
            "Epoch 92/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3513 - accuracy: 0.8450\n",
            "Epoch 00092: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3511 - accuracy: 0.8451 - val_loss: 0.8473 - val_accuracy: 0.5250\n",
            "Epoch 93/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8462\n",
            "Epoch 00093: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3497 - accuracy: 0.8461 - val_loss: 0.8443 - val_accuracy: 0.5250\n",
            "Epoch 94/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8452\n",
            "Epoch 00094: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3502 - accuracy: 0.8445 - val_loss: 0.8674 - val_accuracy: 0.5210\n",
            "Epoch 95/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.8476\n",
            "Epoch 00095: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3481 - accuracy: 0.8465 - val_loss: 0.7887 - val_accuracy: 0.5435\n",
            "Epoch 96/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.8459\n",
            "Epoch 00096: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3484 - accuracy: 0.8456 - val_loss: 0.7356 - val_accuracy: 0.5485\n",
            "Epoch 97/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3458 - accuracy: 0.8480\n",
            "Epoch 00097: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3461 - accuracy: 0.8480 - val_loss: 0.7572 - val_accuracy: 0.5385\n",
            "Epoch 98/100\n",
            "246/250 [============================>.] - ETA: 0s - loss: 0.3469 - accuracy: 0.8485\n",
            "Epoch 00098: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3480 - accuracy: 0.8476 - val_loss: 0.7890 - val_accuracy: 0.5315\n",
            "Epoch 99/100\n",
            "245/250 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8468\n",
            "Epoch 00099: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3458 - accuracy: 0.8468 - val_loss: 0.7409 - val_accuracy: 0.5625\n",
            "Epoch 100/100\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.8475\n",
            "Epoch 00100: val_accuracy did not improve from 0.73350\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.3476 - accuracy: 0.8478 - val_loss: 0.7433 - val_accuracy: 0.5445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxM9BQZvFsKr",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The LTSM model perfomed better than the simple DNN model with the validation accuracy is 73.35%.  However, I expected a validation accuracy above 90%.  It appears to be difficult with the training dataset because it is simply not large enough to produce a model that produce a high validation accuracy.  The final notebook will cover transfer learning."
      ]
    }
  ]
}